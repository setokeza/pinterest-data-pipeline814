{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14425bae-9d1f-4719-b204-e1c8f79efae8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Batch Processing: Spark on Databricks\n",
    "## Mount AWS S3 bucket to Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca343be-7a67-4c42-8da0-1fb912542106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confirm location of authentificationauthentication_credentials.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4dad41c8-066a-4724-8aab-f5560e37ee37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3978eb0d-d9a3-40bb-9c61-e907d1480539",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read the csv file containing the AWS keys to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f8e7c2-821c-494e-b49f-3303c705ac74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3709420-cd8b-4342-8ce2-c6bc9c38ebd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20fbe3e3-878c-4a1b-9946-e906badcc95a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mount the S3 bucket to local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fe83d2-5228-4b2b-8037-67ab76288355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-0ecf5ea19ac5-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/0ecf5ea19ac5_s3_mount\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28520c8-dffb-4326-b243-1cb0ffa92959",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Confirm that we can read data from the mounted S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cf3fe109-d12c-465e-8117-38749933e615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/0ecf5ea19ac5_s3_mount/topics/0ecf5ea19ac5.geo/partition=0/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce10d51-62ac-4404-adb2-05ae901ef001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Create the following three dataframes: \n",
    "## df_pin, df_geo and df_user from S3 data\n",
    "\n",
    "Re-run from this point after loading fresh data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ee046b-38d5-4f2b-bbde-1874bf62fb1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import urllib\n",
    "\n",
    "topics =  {'df_pin':'0ecf5ea19ac5.pin', 'df_geo':'0ecf5ea19ac5.geo', 'df_user':'0ecf5ea19ac5.user'}\n",
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "for df, topic in topics.items():\n",
    "    file_location = f\"/mnt/0ecf5ea19ac5_s3_mount/topics/{topic}/partition=0/*.json\" \n",
    "    file_type = \"json\"\n",
    "    # Ask Spark to infer the schema\n",
    "    infer_schema = \"true\"\n",
    "    # Read in JSONs from mounted S3 bucket\n",
    "    df = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "    # Display Spark dataframe to check its content\n",
    "    if 'pin' in topic:\n",
    "        df_pin = df\n",
    "    elif 'geo' in topic:\n",
    "        df_geo = df\n",
    "    elif 'user' in topic:\n",
    "        df_user = df\n",
    "    display(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6847cc-3de3-4d42-96fe-d524c238a0e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Data cleaning for df_pin<br>\n",
    "\n",
    "- Remove duplicate rows in the dataframe\n",
    "- Rename the column index to ind\n",
    "- Re-order the column names in the dataframe\n",
    "- Replace the values of follower_count column wherever necessary and hence converting the column into a integer data type\n",
    "- Remove any additional strings from the save_location column\n",
    "- Replace all the NA with None\n",
    "- Drop the rows where all columns have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ab4c6d-44f2-4899-99f6-0da7816b1873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_pin data clean\n",
    "df_pin = df_pin.dropDuplicates()\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \n",
    "\"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\", \"downloaded\")\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%k]', '000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%M]', '000000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%User Info Error%]', ''))\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin['follower_count'].cast(IntegerType()))\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace('save_location', 'Local save in *', ''))\n",
    "df_pin.na.fill('None', ['is_image_or_video', 'image_src'])\n",
    "df_pin.na.drop(how = \"all\")\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd88a33-7a22-4d56-8001-aa5776cbb0a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data cleaning for df_geo\n",
    "- Remove duplicate rows in the dataframe\n",
    "- Create new column coordinates with the values to be the array of latitude and longitude column and deleting these two columns\n",
    "- Convert the timestamp column into a timestamp data type\n",
    "- Re-order the column names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000ef518-42c1-4a3f-927b-18d5cfca8e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_geo data clean\n",
    "df_geo = df_geo.dropDuplicates()\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "df_geo = df_geo.withColumn(\"timestamp\", df_geo[\"timestamp\"].cast(TimestampType()))\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d7ef1-ac13-49ae-b4a9-7b790718ccb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data cleaning for df_user\n",
    "- Remove duplicate rows in the dataframe\n",
    "- Create new column user_name by combining the first_name and last_name column and deleting these two columns\n",
    "- Convert the date_joined column into a timestamp data type\n",
    "- Re-order the column names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859d49ab-a545-4b00-b7e1-6ab877e82f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_user data clean\n",
    "df_user = df_user.dropDuplicates()\n",
    "df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "df_user = df_user.withColumn('date_joined', df_user['date_joined'].cast(TimestampType()))\n",
    "df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd8f14b-096a-487e-9452-72642d192fca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pinterest Business Intelligence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cadf0baa-5d4b-4cc2-8287-5795f8735aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Create initial setup for subsequent queries.<br>\n",
    "2. There are two queries per report, one in pyspark SQL and one in regular SQL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10130b6f-3b27-485c-abd5-cb81d31ecc56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_geo.createOrReplaceTempView(\"geo_table\")\n",
    "df_pin.createOrReplaceTempView(\"pin_table\")\n",
    "df_user.createOrReplaceTempView(\"user_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf7e03a-2c0b-4830-9497-731d91f49443",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Find the most popular Pinterest category people post to based on their country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207939c5-92dc-4ed2-aa6e-73c99aacae35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "# create partition by country and order by category_count descending\n",
    "windowCountryByCatCount = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# find the most popular category in each country\n",
    "df_pin.join(df_geo, df_pin.ind == df_geo.ind)\\\n",
    "    .groupBy(\"country\", \"category\") \\\n",
    "    .agg(count(\"category\").alias(\"category_count\")) \\\n",
    "    .withColumn(\"rank\", row_number().over(windowCountryByCatCount)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .drop(\"rank\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe45c4c6-b0b6-4491-97a7-f52aa7684a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#regular sql\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                      \n",
    "    WITH Ranked as (\n",
    "    SELECT \n",
    "        geo_table.country AS country, \n",
    "        pin_table.category AS category, \n",
    "        count(pin_table.category) AS category_count,\n",
    "        ROW_NUMBER() OVER(PARTITION BY geo_table.country ORDER BY count(pin_table.category) DESC) category_rank\n",
    "    FROM \n",
    "        geo_table\n",
    "    INNER JOIN \n",
    "        pin_table ON geo_table.ind = pin_table.ind\n",
    "    GROUP BY \n",
    "        geo_table.country, \n",
    "        pin_table.category \n",
    "    )   \n",
    "        SELECT\n",
    "            country,\n",
    "            category,\n",
    "            category_count\n",
    "        FROM\n",
    "            Ranked\n",
    "        WHERE\n",
    "            category_rank = 1\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330f0142-c1c6-4f75-a943-fa9136dc6a1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Which was the most popular category between 2018 and 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2b092f-eb81-4b8e-aaeb-7e755cbc6714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "# create partition by year and order by category_count descending\n",
    "windowYearByCatCount = Window.partitionBy(\"post_year\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# find which was the most popular category each year between 2018 and 2022\n",
    "df_pin.join(df_geo, df_pin.ind == df_geo.ind)\\\n",
    "    .withColumn(\"post_year\", year(\"timestamp\")) \\\n",
    "    .filter(col(\"post_year\") >= 2018) \\\n",
    "    .filter(col(\"post_year\") <= 2022) \\\n",
    "    .groupBy(\"post_year\", \"category\") \\\n",
    "    .agg(count(\"category\").alias(\"category_count\")) \\\n",
    "    .withColumn(\"rank\", row_number().over(windowYearByCatCount)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .drop(\"rank\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f506f2d-84b5-449e-8769-4af8b7ed8a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# regular SQL\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                      \n",
    "WITH Ranked As (\n",
    "    SELECT DISTINCT \n",
    "        YEAR(geo_table.timestamp) AS post_year, \n",
    "        pin_table.category AS category, \n",
    "        COUNT(pin_table.category) AS category_count,\n",
    "        ROW_NUMBER() OVER(PARTITION BY YEAR(geo_table.timestamp) ORDER BY COUNT(pin_table.category) DESC) category_rank\n",
    "    FROM \n",
    "        geo_table\n",
    "    INNER JOIN \n",
    "        pin_table ON geo_table.ind = pin_table.ind\n",
    "    WHERE \n",
    "        YEAR(geo_table.timestamp) >= 2018 AND YEAR(geo_table.timestamp) <= 2022\n",
    "    GROUP BY \n",
    "        YEAR(geo_table.timestamp), pin_table.category\n",
    ") \n",
    "    SELECT\n",
    "        post_year, category, category_count\n",
    "    FROM\n",
    "        Ranked\n",
    "    WHERE\n",
    "        category_rank == 1\n",
    "        \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e26cab7-479f-4455-a7c7-2b49692d64fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Step 1: For each country find the user with the most followers.<br>\n",
    "   Step 2: Based on the above query, find the country with the user with most followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5618a6-724d-47a9-a354-9e5381098982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "#Find user with most followers in each country\n",
    "# create partition by country and order by follower_count descending\n",
    "windowCountryByFollowers = Window.partitionBy(\"country\").orderBy(col(\"follower_count\").desc())\n",
    "\n",
    "# find the user with the most followers in each country\n",
    "max_followers_by_country = \\\n",
    "    df_pin.join(df_geo, df_pin.ind == df_geo.ind) \\\n",
    "    .withColumn(\"rank\", row_number().over(windowCountryByFollowers)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"country\", \"poster_name\", \"follower_count\") \\\n",
    "    .orderBy(col(\"country\"))\n",
    "\n",
    "# get highest number of followers from all countries\n",
    "max_followers_all_countries = max_followers_by_country.select(max(\"follower_count\")).collect()[0][0]\n",
    "\n",
    "# find the country with the user with most followers\n",
    "country_with_max_followers = \\\n",
    "    max_followers_by_country \\\n",
    "    .select(\"country\",\"follower_count\") \\\n",
    "    .orderBy(col(\"follower_count\").desc()) \\\n",
    "    .limit(1)\n",
    "\n",
    "max_followers_by_country.show()\n",
    "country_with_max_followers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2d7b8d-0291-4866-887c-9b33bb34f374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# regular SQL\n",
    "\n",
    "# STEP 1\n",
    "result_df = spark.sql(\"\"\"\n",
    "                      \n",
    "WITH Ranked AS (\n",
    "    \n",
    "    SELECT \n",
    "        pin_table.poster_name AS poster,\n",
    "        geo_table.country AS country,\n",
    "        pin_table.follower_count AS follower_count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY geo_table.country ORDER BY pin_table.follower_count DESC) AS rank\n",
    "    FROM\n",
    "        geo_table\n",
    "    INNER JOIN pin_table ON geo_table.ind = pin_table.ind\n",
    ")\n",
    "\n",
    "    SELECT\n",
    "        country, poster, follower_count\n",
    "    FROM\n",
    "        Ranked\n",
    "    WHERE\n",
    "        rank = 1\n",
    "    ORDER BY \n",
    "        country;\n",
    "        \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bfb4ca-647c-4217-8fb8-f49bf46d570c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use SQL to join DataFrames: STEP 2\n",
    "result_df = spark.sql(\"\"\"\n",
    "                      \n",
    "WITH Ranked AS (\n",
    "    \n",
    "    SELECT \n",
    "        pin_table.poster_name AS poster,\n",
    "        geo_table.country AS country,\n",
    "        pin_table.follower_count AS follower_count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY geo_table.country ORDER BY pin_table.follower_count DESC) AS rank\n",
    "    FROM\n",
    "        geo_table\n",
    "    INNER JOIN pin_table ON geo_table.ind = pin_table.ind\n",
    ")\n",
    "\n",
    "    SELECT\n",
    "        country, follower_count\n",
    "    FROM\n",
    "        Ranked\n",
    "    WHERE\n",
    "        rank = 1\n",
    "    ORDER BY \n",
    "        follower_count DESC  \n",
    "    LIMIT 1\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c746fc9-5cf9-4b88-ae66-7b1f8e4881fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Find the most popular category people post to, based on the age groups - 18-24, 25-35, 36-50, +50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6d1062-1a1a-41e2-816b-54c4e43cf176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "#Define age groups\n",
    "pin_user_age_group =\\\n",
    "df_pin.join(df_user, 'ind') \\\n",
    "    .withColumn('age_group', expr(\"\"\"case\n",
    "    when age BETWEEN 18 AND 24 THEN '18-24'\n",
    "    when age BETWEEN 25 AND 35 THEN '25-35'\n",
    "    when age BETWEEN 36 AND 50 THEN '36-50'\n",
    "    when age> 50 THEN '+50'\n",
    "    END\n",
    "    \"\"\"))\n",
    "\n",
    "# create partition by age_group and order by category_count descending\n",
    "windowAgeGroup = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# find the most popular category for different age groups\n",
    "pin_user_age_group.groupBy(\"category\",\"age_group\") \\\n",
    "    .agg(count(\"category\").alias(\"category_count\")) \\\n",
    "    .withColumn(\"rank\", row_number().over(windowAgeGroup)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .drop(\"rank\") \\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea129d3a-972f-4d28-b05c-3d16336ab4eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# regular SQL\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "WITH Ranked AS (\n",
    "              \n",
    "    SELECT \n",
    "        pin_table.category as category,\n",
    "        CASE\n",
    "            WHEN user_table.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "            when user_table.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "            when user_table.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "            when user_table.age > 50 THEN '+50'\n",
    "        END as age_group,\n",
    "        COUNT(pin_table.category) AS category_count\n",
    "    FROM\n",
    "        pin_table\n",
    "    INNER JOIN \n",
    "        user_table ON pin_table.ind = user_table.ind   \n",
    "    GROUP BY\n",
    "        pin_table.category, age_group\n",
    "    ORDER BY\n",
    "        age_group, category_count DESC\n",
    "        \n",
    "),\n",
    "\n",
    "Ranked_Window as (\n",
    "    \n",
    "    SELECT\n",
    "        category, age_group, category_count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY age_group ORDER BY category_count DESC) AS rank\n",
    "    FROM\n",
    "        Ranked\n",
    ")\n",
    "\n",
    "    SELECT \n",
    "        category, age_group, category_count\n",
    "    FROM\n",
    "        Ranked_Window\n",
    "    WHERE\n",
    "        rank = 1\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "display(result_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "755ce57d-9a9a-4616-a58f-5e29adcec1fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. Find the median follower count for users in the age groups, 18-24, 25-35, 36-50, +50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09025c4a-c541-4405-a3da-5abe027fa2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "#Find the median follower count for different age groups\n",
    "pin_user_age_group \\\n",
    "    .select(\"age_group\", \"follower_count\") \\\n",
    "    .groupBy(\"age_group\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .orderBy(\"age_group\") \\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e34841-ffa5-4a77-ba44-bef81b03ff26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standard SQL\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                      \n",
    "WITH  CTE AS \n",
    "(\n",
    "    \n",
    "    SELECT \n",
    "        CASE\n",
    "            WHEN user_table.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "            when user_table.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "            when user_table.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "            when user_table.age > 50 THEN '+50'\n",
    "        END as age_group,\n",
    "        pin_table.follower_count as follower_count\n",
    "    FROM\n",
    "        pin_table\n",
    "    INNER JOIN \n",
    "        user_table ON pin_table.ind = user_table.ind\n",
    "    WHERE\n",
    "        pin_table.follower_count IS NOT NULL\n",
    "    ORDER BY\n",
    "        age_group\n",
    "        \n",
    "), \n",
    "    \n",
    "CTE2 AS (\n",
    "    SELECT \n",
    "        CTE.age_group, CTE.follower_count,\n",
    "        NTILE(2) OVER(PARTITION BY CTE.age_group ORDER BY CTE.follower_count) as half1, \n",
    "        NTILE(2) OVER(PARTITION BY CTE.age_group ORDER BY CTE.follower_count DESC) as half2\n",
    "    FROM\n",
    "        CTE\n",
    ")\n",
    "    \n",
    "    SELECT  CTE2.age_group,\n",
    "            ROUND((MAX(CASE WHEN CTE2.half1 = 1 THEN CTE2.follower_count END) + \n",
    "            MIN(CASE WHEN CTE2.half2 = 1 THEN CTE2.follower_count END)) / 2.0) as median_follower_count\n",
    "    FROM    CTE2\n",
    "    GROUP BY CTE2.age_group;\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05eeb19d-9513-4330-b280-a8fb1e443027",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. Find how many users have joined between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d02a48-94c0-4b66-9e63-02144a97934c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "df_user\\\n",
    "    .groupBy(year('date_joined').alias('post_year'))\\\n",
    "    .agg(count_distinct('ind').alias('number_users_joined')) \\\n",
    "    .select('post_year', 'number_users_joined')\\\n",
    "    .where(col('post_year').between('2015', '2020'))\\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14de8801-fc5c-4e98-8b1c-1b8147ebd7a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# standard sql\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "\n",
    "    SELECT DISTINCT\n",
    "        YEAR(date_joined) as post_year,\n",
    "        COUNT (ind) as number_of_users_joined\n",
    "    FROM\n",
    "        user_table\n",
    "    WHERE \n",
    "        YEAR(date_joined) BETWEEN 2015 AND 2020\n",
    "    GROUP BY\n",
    "        post_year\n",
    "    ORDER BY\n",
    "        post_year\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d203b01b-4058-4183-bfdd-3a3de30a307a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. Find the median follower count of users who have joined between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8023b29f-a454-4026-9e70-d7dc5caf9663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "df_user.join(df_pin, 'ind')\\\n",
    "    .groupBy(year('date_joined').alias('post_year'))\\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .select('post_year', 'median_follower_count')\\\n",
    "    .where(col('post_year').between('2015', '2020'))\\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c09604-739f-450e-b8f6-b7520a7196bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# standard sql\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "\n",
    "WITH CTE AS (        \n",
    "    SELECT \n",
    "        YEAR(user_table.date_joined) as post_year,\n",
    "        pin_table.follower_count as follower_count\n",
    "    FROM\n",
    "        user_table\n",
    "    INNER JOIN \n",
    "        pin_table ON user_table.ind = pin_table.ind\n",
    "    WHERE \n",
    "        YEAR(user_table.date_joined) BETWEEN 2015 AND 2020\n",
    "    ORDER BY\n",
    "        post_year\n",
    "),\n",
    "    \n",
    "CTE2 AS (\n",
    "    SELECT \n",
    "        CTE.post_year, CTE.follower_count,\n",
    "        NTILE(2) OVER(PARTITION BY CTE.post_year ORDER BY CTE.follower_count) as half1, \n",
    "        NTILE(2) OVER(PARTITION BY CTE.post_year ORDER BY CTE.follower_count DESC) as half2\n",
    "    FROM\n",
    "        CTE\n",
    ")\n",
    "    \n",
    "    SELECT  CTE2.post_year,\n",
    "            ROUND((MAX(CASE WHEN CTE2.half1 = 1 THEN CTE2.follower_count END) + \n",
    "            MIN(CASE WHEN CTE2.half2 = 1 THEN CTE2.follower_count END)) / 2.0) as median_follower_count\n",
    "    FROM    CTE2\n",
    "    GROUP BY CTE2.post_year;    \n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17dff7ee-e297-44a7-a4f1-aa47daef7769",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. Find the median follower count of users who have joined between 2015 and 2020, based on age group that they are part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a16a84-39e6-46db-a7d1-e0839a214c0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pyspark sql\n",
    "\n",
    "df_pin.join(df_user, 'ind')\\\n",
    "    .withColumn('age_group', expr(\"\"\"case\n",
    "        when age BETWEEN 18 AND 24 THEN '18-24'\n",
    "        when age BETWEEN 25 AND 35 THEN '25-35'\n",
    "        when age BETWEEN 36 AND 50 THEN '36-50'\n",
    "        when age> 50 THEN '+50'\n",
    "        END\n",
    "        \"\"\"))\\\n",
    "    .groupBy(year('date_joined').alias('post_year'), 'age_group')\\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .select('post_year', 'age_group','median_follower_count')\\\n",
    "    .where(col('post_year').between('2015', '2020'))\\\n",
    "    .orderBy('post_year','age_group')\\\n",
    "    .show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce59e02-4398-41d8-ad37-600443bbcd21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#standard sql\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            YEAR(user_table.date_joined) as post_year,\n",
    "            CASE\n",
    "                WHEN user_table.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "                when user_table.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "                when user_table.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "                when user_table.age > 50 THEN '+50'\n",
    "            END as age_group,\n",
    "            pin_table.follower_count as follower_count\n",
    "        FROM\n",
    "            pin_table\n",
    "        INNER JOIN \n",
    "            user_table ON pin_table.ind = user_table.ind\n",
    "        WHERE \n",
    "            YEAR(user_table.date_joined) BETWEEN 2015 AND 2020 \n",
    "        ORDER BY\n",
    "            post_year, age_group\n",
    "     ),\n",
    "    \n",
    "    CTE2 AS (\n",
    "        SELECT \n",
    "            CTE.post_year, CTE.age_group, CTE.follower_count,\n",
    "            NTILE(2) OVER(PARTITION BY CTE.post_year, CTE.age_group  ORDER BY CTE.follower_count) as half1, \n",
    "            NTILE(2) OVER(PARTITION BY CTE.post_year, CTE.age_group ORDER BY CTE.follower_count DESC) as half2\n",
    "        FROM\n",
    "            CTE\n",
    "    )\n",
    "    \n",
    "    SELECT  CTE2.post_year, CTE2.age_group, \n",
    "            ROUND((MAX(CASE WHEN CTE2.half1 = 1 THEN CTE2.follower_count END) + \n",
    "            MIN(CASE WHEN CTE2.half2 = 1 THEN CTE2.follower_count END)) / 2.0) as median_follower_count\n",
    "    FROM    CTE2\n",
    "    GROUP BY CTE2.post_year, CTE2.age_group; \n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "d0ab4c6d-44f2-4899-99f6-0da7816b1873",
       "elementType": "command",
       "guid": "4edceb10-c82b-4361-93bd-1bbf7f5178b4",
       "options": null,
       "position": {
        "height": 6,
        "width": 24,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "674f3494-8983-4d3a-a510-ff1b4eed4efc",
     "origId": 4155037068219074,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
